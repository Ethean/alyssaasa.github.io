<!DOCTYPE html>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
  
  <title>Self-Attention机制 - Sooner or Later, One Way or Another</title>
  <meta charset="UTF-8">
  <meta name="description" content>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  

  <link rel="shortcut icon" href="/favicon.ico" type="image/png">
  <meta name="description" content="Self-Attention机制图像方面：增大图像的感受野 NLP方面：定位关键token或者特征 NLP：Attention Is All You Need1. 高层Transformer我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为：  Transformer的本质上是一个Encoder-Decoder的结构，那么上面图片的结构可以表示">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Self-Attention机制">
<meta property="og:url" content="http://yoursite.com/2019/08/16/Self-Attention机制/index.html">
<meta property="og:site_name" content="Sooner or Later, One Way or Another">
<meta property="og:description" content="Self-Attention机制图像方面：增大图像的感受野 NLP方面：定位关键token或者特征 NLP：Attention Is All You Need1. 高层Transformer我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为：  Transformer的本质上是一个Encoder-Decoder的结构，那么上面图片的结构可以表示">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/self-attention/the_transformer_3.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/The_transformer_encoders_decoders.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/The_transformer_encoder_decoder_stack.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/Transformer_encoder.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/Transformer_decoder.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/embeddings.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/encoder_with_tensors.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/transformer_self-attention_visualization.png">
<meta property="og:image" content="http://yoursite.com/images/self-attention/transformer_self_attention_vectors.png">
<meta property="og:updated_time" content="2019-09-10T17:33:39.857Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Self-Attention机制">
<meta name="twitter:description" content="Self-Attention机制图像方面：增大图像的感受野 NLP方面：定位关键token或者特征 NLP：Attention Is All You Need1. 高层Transformer我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为：  Transformer的本质上是一个Encoder-Decoder的结构，那么上面图片的结构可以表示">
<meta name="twitter:image" content="http://yoursite.com/images/self-attention/the_transformer_3.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/css/mdui.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.15.8/styles/atom-one-dark.css">
   
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1038733_0xvrvpg9c0r.css">
  <link rel="stylesheet" href="/css/style.css?v=1568442393756">
</head>

<body class="mdui-drawer-body-left">
  
  <div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(https://i.loli.net/2019/01/13/5c3aec85a4343.jpg)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">menu</i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/" title="Alyssa" class="mdui-btn mdui-btn-icon"><img src="/images/toux.jpg"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="Alyssa">
            <img src="/images/toux.jpg" alt="Alyssa">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>19</div>
        <div><span>标签</span>6</div>
        <div><span>分类</span>5</div>
    </div>
    <ul class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about.html" title="关于博客">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博客
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
    </ul>
    <aside id="nexmoe-sidebar">
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">社交按钮</h3>
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://space.bilibili.com/20238211" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/alyssaasa/" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章分类</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Hexo/">Hexo</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Shell/">Shell</a>
          <span class="category-list-count">2</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/机器学习/">机器学习</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/深度学习/">深度学习</a>
          <span class="category-list-count">2</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/算法/">算法</a>
          <span class="category-list-count">13</span>
        </li>

        
      </ul>

    </div>
  </div>


  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">标签云</h3>
    <div id="randomtagcloud" class="nexmoe-widget tagcloud">
      <a href="/tags/Github/" style="font-size: 10px;">Github</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/机器学习/" style="font-size: 10px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/算法/" style="font-size: 20px;">算法</a>
    </div>
    
  </div>

  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章归档</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul>
    </div>
  </div>


  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2019 Alyssa
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://nexmoe.com/hexo-theme-nexmoe.html" target="_blank">Nexmoe</a>
    </div>
</div><!-- .nexmoe-drawer -->
  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <div class="nexmoe-post">
    <div class="nexmoe-post-cover"> 
        
            <img src="https://i.loli.net/2019/01/13/5c3aec85a4343.jpg">
        
        <h1>Self-Attention机制</h1>
    </div>
  <div class="nexmoe-post-meta">
    <a><i class="nexmoefont icon-calendar-fill"></i>2019年08月16日</a>
    <a><i class="nexmoefont icon-areachart"></i>691 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 3 分钟</a>
    
      <a class="nexmoefont icon-appstore-fill -link" href="/categories/深度学习/">深度学习</a>
    
    
      <a class="nexmoefont icon-tag-fill -link" href="/tags/深度学习/">深度学习</a>
    
  </div>
  <article>
    <h1 id="Self-Attention机制"><a href="#Self-Attention机制" class="headerlink" title="Self-Attention机制"></a>Self-Attention机制</h1><p>图像方面：增大图像的感受野</p>
<p>NLP方面：定位关键token或者特征</p>
<h3 id="NLP：Attention-Is-All-You-Need"><a href="#NLP：Attention-Is-All-You-Need" class="headerlink" title="NLP：Attention Is All You Need"></a>NLP：Attention Is All You Need</h3><h4 id="1-高层Transformer"><a href="#1-高层Transformer" class="headerlink" title="1. 高层Transformer"></a>1. 高层Transformer</h4><p>我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为：</p>
<p><img src="/images/self-attention/the_transformer_3.png" alt="1"></p>
<p>Transformer的本质上是一个Encoder-Decoder的结构，那么上面图片的结构可以表示为：</p>
<p><img src="/images/self-attention/The_transformer_encoders_decoders.png" alt="2"></p>
<p>Encoders由6个编码block组成，同样Decoders是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入，如图3所示：</p>
<p><img src="/images/self-attention/The_transformer_encoder_decoder_stack.png" alt="3"></p>
<p>6个encoder的结构都是相同的，但是他们不共享权值，每个encoder由两部分组成：self-attention 和 feedd forward neural network:</p>
<p><img src="/images/self-attention/Transformer_encoder.png" alt="4"></p>
<p>Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：</p>
<ul>
<li>Self-Attention：当前翻译和已经翻译的前文之间的关系；</li>
<li>Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。</li>
</ul>
<p><img src="/images/self-attention/Transformer_decoder.png" alt="5"></p>
<h4 id="2-输入编码"><a href="#2-输入编码" class="headerlink" title="2. 输入编码"></a>2. 输入编码</h4><p>上一节主要介绍了Transformer的框架，下面介绍他的输入数据。</p>
<p>如图6所示，首先通过Word2Vec等embedding方法将输入语料转为特征向量：（dimension=512）</p>
<p><img src="/images/self-attention/embeddings.png" alt="6. 单词的输入编码"></p>
<p>embedding只发生在最底层的encoder中，即只有网络的第一层是将embedding的结果输入到网络中，之后每层的输入都是上一层的输出。</p>
<p><img src="/images/self-attention/encoder_with_tensors.png" alt="7"></p>
<p>在这里，我们可以看到Transformer的一个关键属性，即每个位置的单词在编码器中流经它自己的路径。 self-attention层中的这些路径之间存在依赖关系。 然而，feed-forward层不具有那些依赖性，因此各种路径可以在流过前馈层时并行执行。?? 不太懂</p>
<h4 id="3-Self-Attention机制"><a href="#3-Self-Attention机制" class="headerlink" title="3. Self-Attention机制"></a>3. Self-Attention机制</h4><p>self-attention的核心内容是为输入向量的每个单词学习一个权重。</p>
<p>例如我们要翻译的句子是：（即网络模型的输入sentence）<br>”The animal didn’t cross the street because it was too tired”</p>
<p>句子里面的<code>it</code>指代的是谁？animal 还是 street。</p>
<p>Self-attention 就是 Transformer 用来将其他相关单词的“理解”融入我们当前正在处理的单词中的方法。</p>
<p>Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>
<p><img src="/images/self-attention/transformer_self-attention_visualization.png" alt="8. As we are encoding the word &quot;it&quot; in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on &quot;The Animal&quot;, and baked a part of its representation into the encoding of &quot;it&quot;."></p>
<p>在self-attention中，每个单词有3个不同的向量，它们分别是Query向量（ $Q$ ），Key向量（ $K$ ）和Value向量（ $V$ ），长度均是64。它们是通过3个不同的权值矩阵由embedding向量 $X$ 乘以三个不同的权值矩阵 $W^Q$ ， $W^K$ ， $W^V$ 得到，其中三个矩阵的尺寸也是相同的。均是 $512*64$ 。</p>
<p><img src="/images/self-attention/transformer_self_attention_vectors.png" alt="9"></p>
<p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48508221</a></p>
<p><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-transformer/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/64881836" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/64881836</a></p>

  </article>
  
    

  
  <section class="nexmoe-comment">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css">
<div id="gitalk"></div>
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: 'a0b09f47cc93396b2dd6',
        clientSecret: 'bdf8c7211c890d37071003e98dd9879483be6512',
        id: window.location.pathname,
        repo: 'alyssaasa.github.io',
        owner: 'alyssaasa#',
        admin: 'alyssaasa# 拥有对该repo进行操作的 GitHub username'
    })
    gitalk.render('gitalk')
</script>
</section>
</div>
    </div>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/mdui@0.4.3/dist/js/mdui.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
 
    <script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


 
    <script src="https://cdn.jsdelivr.net/npm/smoothscroll-for-websites@1.4.9/SmoothScroll.min.js"></script>


<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script src="/js/app.js?v=1568442393769"></script>
<script src="https://cdn.jsdelivr.net/npm/lazysizes@5.1.0/lazysizes.min.js"></script>


    <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/xtaodada/xtaodada.github.io@0.0.2/copy.js"></script>

  





</body>

</html>
